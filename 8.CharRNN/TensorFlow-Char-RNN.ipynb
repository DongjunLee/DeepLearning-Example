{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Recurrent Neural Networks in Tensorflow II](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Task and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Chip, Kathy, and Nancy, who.....\n",
      "Data length:  4224143\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_url = 'https://raw.githubusercontent.com/samim23/obama-rnn/master/input.txt'\n",
    "file_name = 'obama_speech.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "    \n",
    "with open(file_name, 'r', encoding='utf8') as f:\n",
    "    raw_data = f.read()\n",
    "    print(raw_data[:30] + \".....\")\n",
    "    print(\"Data length: \", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 69, 92, 51, 45, 49, 57, 115, 92, 54]\n"
     ]
    }
   ],
   "source": [
    "# process data - vocab for char and transformed vocab to idx.\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield ptb_iterator(data, batch_size, num_steps)\n",
    "    \n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "    \n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len*i:batch_len*(i+1)]\n",
    "        \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch size == 0, decrease batch_size or num_steps\")\n",
    "        \n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps: (i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1: (i+1)*num_steps+1]\n",
    "        yield (x, y)\n",
    "        \n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def train(g, num_epochs, num_steps=200, batch_size=32, verbose=True, \n",
    "          verbose_step=1000, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        traininig_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            traininig_loss = 0\n",
    "            steps = 0\n",
    "            traininig_state = None\n",
    "            for X, y in epoch:\n",
    "                steps += 1\n",
    "                feed_dict ={g['x']: X, g['y']: y}\n",
    "                if traininig_state is not None:\n",
    "                    feed_dict[g['init_state']] = traininig_state\n",
    "                traininig_loss_, traininig_state, _ = sess.run(\n",
    "                    [g['total_loss'], g['final_state'], g['train_step']], feed_dict)\n",
    "                traininig_loss += traininig_loss_\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Average trainig loss for Epoch\", idx, \":\", traininig_loss/steps)\n",
    "            traininig_losses.append(traininig_loss/steps)\n",
    "            \n",
    "        if isinstance(save, str):\n",
    "            save_dir = save.split(\"/\")[0]\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            g['saver'].save(sess, save)\n",
    "    \n",
    "    return traininig_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(state_size=100, num_classes=vocab_size, batch_size=32, num_steps=200,\n",
    "                num_layers=3, learning_rate=1e-4):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placehoder')\n",
    "    \n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, X)\n",
    "    \n",
    "    cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "    # reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "    \n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    return {\n",
    "        \"x\": X,\n",
    "        \"y\": y,\n",
    "        \"init_state\": init_state,\n",
    "        \"final_state\": final_state,\n",
    "        \"total_loss\": total_loss,\n",
    "        \"train_step\": train_step,\n",
    "        \"prediction\": predictions,\n",
    "        \"saver\": tf.train.Saver()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average trainig loss for Epoch 0 : 2.52731376926\n",
      "Average trainig loss for Epoch 1 : 1.96453432134\n",
      "Average trainig loss for Epoch 2 : 1.76912310658\n",
      "Average trainig loss for Epoch 3 : 1.64480506048\n",
      "Average trainig loss for Epoch 4 : 1.55989130309\n",
      "Average trainig loss for Epoch 5 : 1.49849027399\n",
      "Average trainig loss for Epoch 6 : 1.45135068778\n",
      "Average trainig loss for Epoch 7 : 1.41417253007\n",
      "Average trainig loss for Epoch 8 : 1.38430166599\n",
      "Average trainig loss for Epoch 9 : 1.35971391667\n",
      "It took 2985.2912950515747 seconds to train for 10 epochs.\n",
      "The average loss on the final epoch was: 1.35971391667\n"
     ]
    }
   ],
   "source": [
    "train_epochs = 10\n",
    "train_num_steps = 40\n",
    "checkpoint_dir = \"save/GRU_\" + str(train_epochs) + \"_epochs\"\n",
    "\n",
    "g = build_graph(num_steps=train_num_steps)\n",
    "t = time.time()\n",
    "\n",
    "losses = train(g, train_epochs, num_steps=train_num_steps, save=checkpoint_dir)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for \" + str(train_epochs) + \" epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    # Accepts a current character, inital state\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['prediction'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    return(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H Wasels. They are not because the primately of alliance to came secure with change. All the prises and technology that we will be an accession to combight this its accupled and chould and we’re compenity and shave to broken a dollan entama of the right of the partners of these some for to should be configinc to partners and statism. And what they are without ship as a come that a security to should be the pates and shis sour who are to prayer singa and the right to the some of the pact of a should all a will act of as we can be new presentice in our competiers and sense of you to say that we’ve actions and was so the pass than we see in one travel countrys are comests and who can stand with the way outsigess to do white human tries of the p'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = build_graph(num_steps=1, batch_size=1)\n",
    "generate_characters(g, checkpoint_dir, 750, prompt='H', pick_top_chars=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
