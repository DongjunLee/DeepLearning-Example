{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Convolutional Neural Network for Text Classification in Tensorflow](https://github.com/dennybritz/cnn-text-classification-tf) by dennybritz\n",
    "- [WILDML - IMPLEMENTING A CNN FOR TEXT CLASSIFICATION IN TENSORFLOW](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n",
    "- [WILDML - UNDERSTANDING CONVOLUTIONAL NEURAL NETWORKS FOR NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_path = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "neg_path = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "train_count = 8500\n",
    "\n",
    "x_text, y = load_data_and_labels(pos_path, neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 8500/2162\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "# VocabularyProcessor -> Maps documents to sequences of word ids.\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Random shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "train_X, train_y = x_shuffled[:train_count], y_shuffled[:train_count]\n",
    "dev_X, dev_y = x_shuffled[train_count:], y_shuffled[train_count:]\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(train_y), len(dev_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total count : 8500\n",
      "Sequence Length: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4719,   59,  182,   34,  190,  804,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train total count : \" + str(len(train_X)))\n",
    "print(\"Sequence Length: \" + str(train_X.shape[1]))\n",
    "train_X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                 embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        \n",
    "        # Input Placeholder\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        # Convolution & Max-Pooling Layers for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded, \n",
    "                    W, \n",
    "                    strides=[1,1,1,1], \n",
    "                    padding=\"VALID\", \n",
    "                    name=\"conv\"\n",
    "                )\n",
    "                \n",
    "                # Apply Relu\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # Max-pooling\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h, \n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\"\n",
    "                )\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # Dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        # Score and Predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer()\n",
    "            )\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        # Loss and Accuracy - Calcuate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + (l2_reg_lambda * l2_loss)\n",
    "            \n",
    "        # Calculate Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\", name=\"accuracy\"))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size)\n",
    "    print(num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "model_config = {\n",
    "    \"sequence_length\": train_X.shape[1],\n",
    "    \"num_classes\": 2,\n",
    "    \"vocab_size\": len(vocab_processor.vocabulary_),\n",
    "    \"embedding_size\": 128,\n",
    "    \"filter_sizes\": [3, 4, 5],\n",
    "    \"num_filters\": 128\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    \"dropout_keep_prob\": 0.5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 150,\n",
    "    \"num_checkpoints\": 5,\n",
    "    \"evaluate_every\": 1000,\n",
    "    \"checkpoint_every\": 1000,\n",
    "    \"verbose_step\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=True,\n",
    "          log_device_placement=False\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "\n",
    "        with sess.as_default():\n",
    "            text_cnn = TextCNN(**model_config)\n",
    "\n",
    "            # Define Training procedure Adam optiminzer\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(train_config[\"learning_rate\"])\n",
    "            grads_and_vars = optimizer.compute_gradients(text_cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", text_cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", text_cnn.accuracy)\n",
    "\n",
    "            # Summary - TensorBoard\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "            \n",
    "            # Checkpointing - Tensorflow assumes thks directory already exists so we need to create it\n",
    "            out_dir = \"text_cnn\"\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=train_config[\"num_checkpoints\"])\n",
    "\n",
    "            # Init\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # train_step\n",
    "            def train_step(batch_x, batch_y):\n",
    "                # A Single training step\n",
    "                feed_dict = {\n",
    "                    text_cnn.input_x: batch_x,\n",
    "                    text_cnn.input_y: batch_y,\n",
    "                    text_cnn.dropout_keep_prob: train_config[\"dropout_keep_prob\"]\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, text_cnn.loss, text_cnn.accuracy],\n",
    "                    feed_dict\n",
    "                )\n",
    "\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                if step % train_config[\"verbose_step\"] == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "            def dev_step(batch_x, batch_y, writer=None):\n",
    "                # Evaluates model on a dev set\n",
    "                feed_dict = {\n",
    "                    text_cnn.input_x: batch_x,\n",
    "                    text_cnn.input_y: batch_y,\n",
    "                    text_cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, text_cnn.loss, text_cnn.accuracy],\n",
    "                    feed_dict\n",
    "                )\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "                \n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(train_X, train_y)), train_config[\"batch_size\"], train_config[\"num_epochs\"])\n",
    "\n",
    "            # Trainig loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                \n",
    "                if current_step % train_config[\"evaluate_every\"] == 0:\n",
    "                    print(\"\\nEvalutaion:\")\n",
    "                    dev_step(dev_X, dev_y, writer=dev_summary_writer)\n",
    "                \n",
    "                if current_step % train_config[\"checkpoint_every\"] == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(checkpoint_prefix + \"-\" + str(current_step)))\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/runs/1488877820\n",
      "\n",
      "132\n",
      "2017-03-07T18:10:35.522228: step 100, loss 1.88922, acc 0.5\n",
      "2017-03-07T18:10:49.592683: step 200, loss 1.35667, acc 0.546875\n",
      "2017-03-07T18:11:04.216053: step 300, loss 1.63537, acc 0.5\n",
      "2017-03-07T18:11:18.495420: step 400, loss 1.38577, acc 0.578125\n",
      "2017-03-07T18:11:32.466313: step 500, loss 1.31505, acc 0.65625\n",
      "2017-03-07T18:11:45.762954: step 600, loss 1.51646, acc 0.515625\n",
      "2017-03-07T18:12:00.418741: step 700, loss 1.30144, acc 0.5\n",
      "2017-03-07T18:12:14.504270: step 800, loss 1.18187, acc 0.578125\n",
      "2017-03-07T18:12:27.337939: step 900, loss 1.50026, acc 0.453125\n",
      "2017-03-07T18:12:40.480263: step 1000, loss 1.09362, acc 0.671875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:12:41.576591: step 1000, loss 0.698792, acc 0.59482\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-1000\n",
      "\n",
      "2017-03-07T18:12:55.912384: step 1100, loss 1.11949, acc 0.578125\n",
      "2017-03-07T18:13:09.023309: step 1200, loss 0.854862, acc 0.625\n",
      "2017-03-07T18:13:22.794488: step 1300, loss 1.04179, acc 0.625\n",
      "2017-03-07T18:13:36.180564: step 1400, loss 1.00515, acc 0.703125\n",
      "2017-03-07T18:13:50.144815: step 1500, loss 1.14127, acc 0.640625\n",
      "2017-03-07T18:14:04.420937: step 1600, loss 0.900348, acc 0.578125\n",
      "2017-03-07T18:14:17.966385: step 1700, loss 0.79643, acc 0.671875\n",
      "2017-03-07T18:14:31.181213: step 1800, loss 0.951323, acc 0.515625\n",
      "2017-03-07T18:14:44.572009: step 1900, loss 0.706244, acc 0.671875\n",
      "2017-03-07T18:14:58.611497: step 2000, loss 0.734875, acc 0.65625\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:15:00.270916: step 2000, loss 0.646462, acc 0.62766\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-2000\n",
      "\n",
      "2017-03-07T18:15:15.201200: step 2100, loss 0.865407, acc 0.609375\n",
      "2017-03-07T18:15:28.926545: step 2200, loss 0.796879, acc 0.6875\n",
      "2017-03-07T18:15:42.367280: step 2300, loss 0.866574, acc 0.546875\n",
      "2017-03-07T18:15:56.286963: step 2400, loss 0.461661, acc 0.75\n",
      "2017-03-07T18:16:10.635508: step 2500, loss 0.687615, acc 0.65625\n",
      "2017-03-07T18:16:24.597379: step 2600, loss 0.756752, acc 0.5625\n",
      "2017-03-07T18:16:38.901507: step 2700, loss 0.689318, acc 0.75\n",
      "2017-03-07T18:16:53.884217: step 2800, loss 0.70685, acc 0.640625\n",
      "2017-03-07T18:17:07.408344: step 2900, loss 0.737474, acc 0.625\n",
      "2017-03-07T18:17:21.493710: step 3000, loss 0.509664, acc 0.796875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:17:22.592606: step 3000, loss 0.626867, acc 0.644311\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-3000\n",
      "\n",
      "2017-03-07T18:17:36.574793: step 3100, loss 0.571102, acc 0.75\n",
      "2017-03-07T18:17:50.424362: step 3200, loss 0.702351, acc 0.609375\n",
      "2017-03-07T18:18:03.290823: step 3300, loss 0.673698, acc 0.65625\n",
      "2017-03-07T18:18:16.193567: step 3400, loss 0.624399, acc 0.6875\n",
      "2017-03-07T18:18:31.546849: step 3500, loss 0.585195, acc 0.65625\n",
      "2017-03-07T18:18:45.137459: step 3600, loss 0.543108, acc 0.78125\n",
      "2017-03-07T18:18:58.508021: step 3700, loss 0.536062, acc 0.703125\n",
      "2017-03-07T18:19:12.474539: step 3800, loss 0.520613, acc 0.671875\n",
      "2017-03-07T18:19:26.164571: step 3900, loss 0.555131, acc 0.71875\n",
      "2017-03-07T18:19:39.387391: step 4000, loss 0.628474, acc 0.671875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:19:40.389154: step 4000, loss 0.612269, acc 0.655874\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-4000\n",
      "\n",
      "2017-03-07T18:19:54.570913: step 4100, loss 0.333038, acc 0.890625\n",
      "2017-03-07T18:20:09.882915: step 4200, loss 0.455303, acc 0.84375\n",
      "2017-03-07T18:20:30.616120: step 4300, loss 0.541549, acc 0.71875\n",
      "2017-03-07T18:20:48.453674: step 4400, loss 0.494373, acc 0.828125\n",
      "2017-03-07T18:21:08.323334: step 4500, loss 0.491599, acc 0.75\n",
      "2017-03-07T18:21:26.809682: step 4600, loss 0.517732, acc 0.6875\n",
      "2017-03-07T18:21:44.788560: step 4700, loss 0.494646, acc 0.78125\n",
      "2017-03-07T18:22:04.694527: step 4800, loss 0.561817, acc 0.734375\n",
      "2017-03-07T18:22:26.988681: step 4900, loss 0.431671, acc 0.75\n",
      "2017-03-07T18:22:47.287887: step 5000, loss 0.482164, acc 0.796875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:22:48.760181: step 5000, loss 0.603913, acc 0.674838\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-5000\n",
      "\n",
      "2017-03-07T18:23:08.286972: step 5100, loss 0.503075, acc 0.78125\n",
      "2017-03-07T18:23:26.497151: step 5200, loss 0.433744, acc 0.796875\n",
      "2017-03-07T18:23:48.416158: step 5300, loss 0.38452, acc 0.828125\n",
      "2017-03-07T18:24:09.347735: step 5400, loss 0.390471, acc 0.84375\n",
      "2017-03-07T18:24:27.003398: step 5500, loss 0.5188, acc 0.734375\n",
      "2017-03-07T18:24:44.503612: step 5600, loss 0.400124, acc 0.796875\n",
      "2017-03-07T18:25:04.881283: step 5700, loss 0.545929, acc 0.75\n",
      "2017-03-07T18:25:24.303819: step 5800, loss 0.373283, acc 0.84375\n",
      "2017-03-07T18:25:42.666491: step 5900, loss 0.340091, acc 0.796875\n",
      "2017-03-07T18:26:02.158881: step 6000, loss 0.423898, acc 0.78125\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:26:03.567595: step 6000, loss 0.586306, acc 0.678538\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-6000\n",
      "\n",
      "2017-03-07T18:26:23.873072: step 6100, loss 0.338447, acc 0.859375\n",
      "2017-03-07T18:26:42.085698: step 6200, loss 0.443478, acc 0.8125\n",
      "2017-03-07T18:27:02.999017: step 6300, loss 0.423956, acc 0.796875\n",
      "2017-03-07T18:27:23.127013: step 6400, loss 0.371525, acc 0.859375\n",
      "2017-03-07T18:27:41.804955: step 6500, loss 0.462492, acc 0.765625\n",
      "2017-03-07T18:28:01.812658: step 6600, loss 0.268526, acc 0.9375\n",
      "2017-03-07T18:28:24.969388: step 6700, loss 0.384731, acc 0.859375\n",
      "2017-03-07T18:28:42.977492: step 6800, loss 0.382844, acc 0.828125\n",
      "2017-03-07T18:29:00.998656: step 6900, loss 0.243058, acc 0.90625\n",
      "2017-03-07T18:29:18.912369: step 7000, loss 0.233995, acc 0.90625\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:29:20.234015: step 7000, loss 0.583813, acc 0.686401\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-7000\n",
      "\n",
      "2017-03-07T18:29:38.711868: step 7100, loss 0.347539, acc 0.828125\n",
      "2017-03-07T18:29:56.295211: step 7200, loss 0.284893, acc 0.875\n",
      "2017-03-07T18:30:14.057713: step 7300, loss 0.229796, acc 0.890625\n",
      "2017-03-07T18:30:31.493690: step 7400, loss 0.340973, acc 0.828125\n",
      "2017-03-07T18:30:48.893740: step 7500, loss 0.444301, acc 0.8125\n",
      "2017-03-07T18:31:06.098343: step 7600, loss 0.237724, acc 0.90625\n",
      "2017-03-07T18:31:23.640212: step 7700, loss 0.29585, acc 0.859375\n",
      "2017-03-07T18:31:41.066959: step 7800, loss 0.286225, acc 0.875\n",
      "2017-03-07T18:31:58.530839: step 7900, loss 0.213403, acc 0.90625\n",
      "2017-03-07T18:32:16.054433: step 8000, loss 0.226622, acc 0.921875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:32:17.431428: step 8000, loss 0.578346, acc 0.705828\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-8000\n",
      "\n",
      "2017-03-07T18:32:35.890431: step 8100, loss 0.321581, acc 0.828125\n",
      "2017-03-07T18:32:53.630732: step 8200, loss 0.26699, acc 0.875\n",
      "2017-03-07T18:33:11.434155: step 8300, loss 0.212772, acc 0.90625\n",
      "2017-03-07T18:33:29.222897: step 8400, loss 0.31784, acc 0.859375\n",
      "2017-03-07T18:33:47.320256: step 8500, loss 0.19814, acc 0.90625\n",
      "2017-03-07T18:34:04.798027: step 8600, loss 0.242376, acc 0.90625\n",
      "2017-03-07T18:34:22.426889: step 8700, loss 0.374208, acc 0.875\n",
      "2017-03-07T18:34:39.747286: step 8800, loss 0.189767, acc 0.953125\n",
      "2017-03-07T18:34:57.319745: step 8900, loss 0.195598, acc 0.90625\n",
      "2017-03-07T18:35:14.900876: step 9000, loss 0.283847, acc 0.890625\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:35:16.165863: step 9000, loss 0.587234, acc 0.710453\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-9000\n",
      "\n",
      "2017-03-07T18:35:34.492086: step 9100, loss 0.119235, acc 0.96875\n",
      "2017-03-07T18:35:51.975401: step 9200, loss 0.157819, acc 0.953125\n",
      "2017-03-07T18:36:09.665705: step 9300, loss 0.200218, acc 0.921875\n",
      "2017-03-07T18:36:26.989631: step 9400, loss 0.186491, acc 0.90625\n",
      "2017-03-07T18:36:44.113721: step 9500, loss 0.232259, acc 0.90625\n",
      "2017-03-07T18:37:01.620315: step 9600, loss 0.129033, acc 0.953125\n",
      "2017-03-07T18:37:19.828825: step 9700, loss 0.206179, acc 0.9375\n",
      "2017-03-07T18:37:37.229297: step 9800, loss 0.147412, acc 0.984375\n",
      "2017-03-07T18:37:54.707220: step 9900, loss 0.163407, acc 0.9375\n",
      "2017-03-07T18:38:12.505600: step 10000, loss 0.175191, acc 0.9375\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:38:13.813156: step 10000, loss 0.602509, acc 0.722017\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-10000\n",
      "\n",
      "2017-03-07T18:38:32.310979: step 10100, loss 0.174893, acc 0.9375\n",
      "2017-03-07T18:38:49.763580: step 10200, loss 0.169835, acc 0.921875\n",
      "2017-03-07T18:39:07.611067: step 10300, loss 0.20681, acc 0.953125\n",
      "2017-03-07T18:39:25.178943: step 10400, loss 0.118244, acc 0.96875\n",
      "2017-03-07T18:39:42.846504: step 10500, loss 0.120233, acc 0.96875\n",
      "2017-03-07T18:40:00.878004: step 10600, loss 0.153566, acc 0.953125\n",
      "2017-03-07T18:40:18.411865: step 10700, loss 0.091256, acc 0.984375\n",
      "2017-03-07T18:40:36.165053: step 10800, loss 0.112744, acc 0.96875\n",
      "2017-03-07T18:40:53.955706: step 10900, loss 0.230505, acc 0.921875\n",
      "2017-03-07T18:41:11.936045: step 11000, loss 0.177772, acc 0.921875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:41:13.191996: step 11000, loss 0.62793, acc 0.720629\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-11000\n",
      "\n",
      "2017-03-07T18:41:31.458103: step 11100, loss 0.153206, acc 0.96875\n",
      "2017-03-07T18:41:49.198045: step 11200, loss 0.134184, acc 0.9375\n",
      "2017-03-07T18:42:06.754667: step 11300, loss 0.118623, acc 0.96875\n",
      "2017-03-07T18:42:24.405508: step 11400, loss 0.0924357, acc 0.953125\n",
      "2017-03-07T18:42:42.084060: step 11500, loss 0.0909892, acc 1\n",
      "2017-03-07T18:42:59.746099: step 11600, loss 0.0939976, acc 0.96875\n",
      "2017-03-07T18:43:17.439179: step 11700, loss 0.145205, acc 0.9375\n",
      "2017-03-07T18:43:35.044649: step 11800, loss 0.0979899, acc 0.953125\n",
      "2017-03-07T18:43:53.065902: step 11900, loss 0.290659, acc 0.921875\n",
      "2017-03-07T18:44:11.004835: step 12000, loss 0.113478, acc 0.953125\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:44:12.217882: step 12000, loss 0.652958, acc 0.726642\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-12000\n",
      "\n",
      "2017-03-07T18:44:30.476830: step 12100, loss 0.123699, acc 0.9375\n",
      "2017-03-07T18:44:48.014475: step 12200, loss 0.0544753, acc 0.984375\n",
      "2017-03-07T18:45:06.032386: step 12300, loss 0.107854, acc 0.96875\n",
      "2017-03-07T18:45:24.170878: step 12400, loss 0.123731, acc 0.9375\n",
      "2017-03-07T18:45:41.616441: step 12500, loss 0.0867739, acc 0.984375\n",
      "2017-03-07T18:45:59.260065: step 12600, loss 0.0964605, acc 0.96875\n",
      "2017-03-07T18:46:16.586306: step 12700, loss 0.0823698, acc 0.984375\n",
      "2017-03-07T18:46:33.959295: step 12800, loss 0.0428709, acc 1\n",
      "2017-03-07T18:46:51.447937: step 12900, loss 0.0473391, acc 0.984375\n",
      "2017-03-07T18:47:08.934907: step 13000, loss 0.0542791, acc 0.984375\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:47:10.183649: step 13000, loss 0.681904, acc 0.72988\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-13000\n",
      "\n",
      "2017-03-07T18:47:29.123965: step 13100, loss 0.0731296, acc 0.984375\n",
      "2017-03-07T18:47:46.641844: step 13200, loss 0.110544, acc 0.96875\n",
      "2017-03-07T18:48:04.334960: step 13300, loss 0.0993209, acc 0.96875\n",
      "2017-03-07T18:48:23.156140: step 13400, loss 0.0484663, acc 0.984375\n",
      "2017-03-07T18:48:46.242801: step 13500, loss 0.0750116, acc 0.984375\n",
      "2017-03-07T18:49:09.041123: step 13600, loss 0.0895333, acc 0.96875\n",
      "2017-03-07T18:49:32.258143: step 13700, loss 0.100434, acc 0.953125\n",
      "2017-03-07T18:49:54.211751: step 13800, loss 0.0928274, acc 0.96875\n",
      "2017-03-07T18:50:16.309400: step 13900, loss 0.0650782, acc 0.984375\n",
      "2017-03-07T18:50:38.491232: step 14000, loss 0.0776985, acc 0.984375\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:50:40.057132: step 14000, loss 0.724027, acc 0.738205\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-14000\n",
      "\n",
      "2017-03-07T18:51:04.077501: step 14100, loss 0.0668676, acc 0.96875\n",
      "2017-03-07T18:51:27.774715: step 14200, loss 0.0980153, acc 0.96875\n",
      "2017-03-07T18:51:48.721026: step 14300, loss 0.0685359, acc 0.96875\n",
      "2017-03-07T18:52:09.976329: step 14400, loss 0.0397052, acc 0.984375\n",
      "2017-03-07T18:52:32.089629: step 14500, loss 0.055748, acc 0.96875\n",
      "2017-03-07T18:52:56.695260: step 14600, loss 0.045042, acc 0.984375\n",
      "2017-03-07T18:53:21.389004: step 14700, loss 0.0549164, acc 0.984375\n",
      "2017-03-07T18:53:43.712427: step 14800, loss 0.0527768, acc 0.96875\n",
      "2017-03-07T18:54:04.688425: step 14900, loss 0.0502243, acc 0.984375\n",
      "2017-03-07T18:54:27.022331: step 15000, loss 0.0808123, acc 0.96875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:54:28.730541: step 15000, loss 0.761598, acc 0.734043\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-15000\n",
      "\n",
      "2017-03-07T18:54:53.083250: step 15100, loss 0.0319697, acc 1\n",
      "2017-03-07T18:55:15.001141: step 15200, loss 0.0425688, acc 1\n",
      "2017-03-07T18:55:37.105811: step 15300, loss 0.0723533, acc 0.96875\n",
      "2017-03-07T18:56:00.323580: step 15400, loss 0.0444253, acc 1\n",
      "2017-03-07T18:56:25.183278: step 15500, loss 0.0405279, acc 0.984375\n",
      "2017-03-07T18:56:47.230328: step 15600, loss 0.0296365, acc 1\n",
      "2017-03-07T18:57:08.888374: step 15700, loss 0.0434433, acc 0.984375\n",
      "2017-03-07T18:57:30.879022: step 15800, loss 0.0593475, acc 0.984375\n",
      "2017-03-07T18:57:54.244184: step 15900, loss 0.00738106, acc 1\n",
      "2017-03-07T18:58:15.870099: step 16000, loss 0.0386598, acc 0.984375\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T18:58:17.478701: step 16000, loss 0.797128, acc 0.736818\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-16000\n",
      "\n",
      "2017-03-07T18:58:40.430474: step 16100, loss 0.0351687, acc 0.984375\n",
      "2017-03-07T18:59:02.937104: step 16200, loss 0.0331483, acc 0.984375\n",
      "2017-03-07T18:59:26.992007: step 16300, loss 0.0559847, acc 0.96875\n",
      "2017-03-07T18:59:50.792551: step 16400, loss 0.115777, acc 0.96875\n",
      "2017-03-07T19:00:15.717761: step 16500, loss 0.0224733, acc 1\n",
      "2017-03-07T19:00:38.826940: step 16600, loss 0.0149197, acc 1\n",
      "2017-03-07T19:01:00.369728: step 16700, loss 0.0166729, acc 1\n",
      "2017-03-07T19:01:22.428927: step 16800, loss 0.0323474, acc 0.984375\n",
      "2017-03-07T19:01:46.343254: step 16900, loss 0.0330946, acc 1\n",
      "2017-03-07T19:02:09.730550: step 17000, loss 0.0576353, acc 0.96875\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T19:02:11.761583: step 17000, loss 0.835929, acc 0.734505\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-17000\n",
      "\n",
      "2017-03-07T19:02:38.703444: step 17100, loss 0.0144611, acc 1\n",
      "2017-03-07T19:03:04.771999: step 17200, loss 0.0272379, acc 0.984375\n",
      "2017-03-07T19:03:29.852221: step 17300, loss 0.0266007, acc 1\n",
      "2017-03-07T19:03:55.883736: step 17400, loss 0.0230416, acc 1\n",
      "2017-03-07T19:04:19.666129: step 17500, loss 0.00751897, acc 1\n",
      "2017-03-07T19:04:42.354927: step 17600, loss 0.0119906, acc 1\n",
      "2017-03-07T19:05:03.644679: step 17700, loss 0.00773728, acc 1\n",
      "2017-03-07T19:05:25.022778: step 17800, loss 0.0277083, acc 0.984375\n",
      "2017-03-07T19:05:46.863518: step 17900, loss 0.0183099, acc 1\n",
      "2017-03-07T19:06:10.005373: step 18000, loss 0.0421916, acc 0.984375\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T19:06:12.029642: step 18000, loss 0.879336, acc 0.740056\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-18000\n",
      "\n",
      "2017-03-07T19:06:37.633835: step 18100, loss 0.0442831, acc 0.984375\n",
      "2017-03-07T19:07:01.740718: step 18200, loss 0.0292586, acc 0.984375\n",
      "2017-03-07T19:07:25.284380: step 18300, loss 0.0216572, acc 1\n",
      "2017-03-07T19:07:47.676911: step 18400, loss 0.00750958, acc 1\n",
      "2017-03-07T19:08:11.888100: step 18500, loss 0.0177011, acc 0.984375\n",
      "2017-03-07T19:08:34.709405: step 18600, loss 0.0354292, acc 0.984375\n",
      "2017-03-07T19:08:56.862916: step 18700, loss 0.0105166, acc 1\n",
      "2017-03-07T19:09:21.263959: step 18800, loss 0.00382841, acc 1\n",
      "2017-03-07T19:09:45.822378: step 18900, loss 0.0148757, acc 1\n",
      "2017-03-07T19:10:08.706184: step 19000, loss 0.0168153, acc 1\n",
      "\n",
      "Evalutaion:\n",
      "2017-03-07T19:10:10.320388: step 19000, loss 0.920769, acc 0.745606\n",
      "Saved model checkpoint to /Users/konolabs/Projects/DeepLearning-Notebooks/7.TextCNN/text_cnn/checkpoints/model-19000\n",
      "\n",
      "2017-03-07T19:10:34.862345: step 19100, loss 0.0175119, acc 0.984375\n",
      "2017-03-07T19:11:00.104510: step 19200, loss 0.00916086, acc 1\n",
      "2017-03-07T19:11:23.949480: step 19300, loss 0.00817236, acc 1\n",
      "2017-03-07T19:11:46.298187: step 19400, loss 0.00995565, acc 1\n",
      "2017-03-07T19:12:07.847825: step 19500, loss 0.0119993, acc 1\n",
      "2017-03-07T19:12:28.865143: step 19600, loss 0.0217259, acc 0.984375\n",
      "2017-03-07T19:12:50.373934: step 19700, loss 0.046584, acc 0.984375\n",
      "2017-03-07T19:13:11.266304: step 19800, loss 0.0131077, acc 1\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_x = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "\n",
    "test_X = x_shuffled\n",
    "test_y = y_shuffled\n",
    "\n",
    "eval_config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"checkpoint_dir\": \"./text_cnn/checkpoints/\",\n",
    "    \"eval_train\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalutaion():\n",
    "    checkpoint_file = tf.train.latest_checkpoint(eval_config[\"checkpoint_dir\"])\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            # Load the saved meta graph and restore variables\n",
    "            saver = tf.train.import_meta_graph(str(checkpoint_file) + \".meta\")\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            \n",
    "            # Get the placeholders from the graph by name\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "            \n",
    "            # Tensors we want to evalutae\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "            \n",
    "            # Generate batches for one epoch\n",
    "            batches = batch_iter(test_X, eval_config[\"batch_size\"], 1, shuffle=False)\n",
    "            \n",
    "            # Collect the predictions here\n",
    "            all_predictions = []\n",
    "            \n",
    "            for batch_test_x in batches:\n",
    "                batch_predictions = sess.run(\n",
    "                    predictions,\n",
    "                    {input_x: batch_test_x, dropout_keep_prob: 1.0}\n",
    "                )\n",
    "                all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "                \n",
    "    # Print accuracy if test_y is defind\n",
    "    if test_y is not None:\n",
    "        correct_predictions = 0\n",
    "        for prediction, y in zip(all_predictions, test_y):\n",
    "            if y[int(prediction)] == 1:\n",
    "                correct_predictions += 1 \n",
    "        print(\"Total number of test examples: {}\".format(len(test_y)))\n",
    "        print(\"Accuracy: {:g}\".format(correct_predictions/float(len(test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "Total number of test examples: 10662\n",
      "Accuracy: 0.946164\n"
     ]
    }
   ],
   "source": [
    "evalutaion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Kono)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
